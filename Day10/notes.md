
---

### **Day10/notes.md**
```markdown
# Day 10 â€“ Project #2: Titanic Survivor Prediction

---

## ğŸ¯ Goals for the Day
- Build multiple classification models on the Titanic dataset.
- Compare their performance.

---

## ğŸ“ Detailed Summary
Todayâ€™s project was about predicting whether a passenger survived the Titanic disaster.  

Steps:
1. Loaded Titanic dataset from Kaggle.
2. Cleaned missing values (`Age`, `Embarked`).
3. Encoded categorical features using OneHotEncoder.
4. Trained Logistic Regression, Random Forest, and XGBoost.
5. Compared models using accuracy and ROC-AUC score.

---

## ğŸ”‘ Key Concepts Learned
- **Feature Engineering**: Creating `FamilySize` from `SibSp` and `Parch`.
- **Encoding**: Converting categorical variables into numeric form.
- **Model Comparison**: Using multiple metrics, not just accuracy.

---

## ğŸ’» Code Snippet
```python
from sklearn.metrics import roc_auc_score

for model in [log_reg, forest, xgb]:
    y_pred = model.predict(X_test)
    print(model.__class__.__name__, "AUC:", roc_auc_score(y_test, y_pred))
